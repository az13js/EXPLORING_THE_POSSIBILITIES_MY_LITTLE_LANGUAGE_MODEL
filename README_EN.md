# Exploring the possibilities: My LLM (little language model)

[中文](./README.md)

LLM is a powerful tool for language processing. Recently, I used the torch
library to write a neural network model and tried to find new possibilities
in the field of NLP.

This model is very simple, using only a fully-connected neural network,
but its potential cannot be underestimated. The fully-connected neural network
is like a detailed spider web, where every small signal from one place can be
broadcast to all locations - every neuron can receive global information as
input. The final combined model belongs to the RNN model, which has unique
advantages in processing sequence data, especially for language as a kind of
sequence information.

What's more, this model is very small in size, which saves a lot of memory and
CPU power. This makes it capable of processing large-scale language data at a
very fast speed. In this era of big data, this lightweight model is undoubtedly
an ideal choice for many application scenarios that save memory and
computing power.

The birth of the model is thanks to BELLE corpus. Thank you for the efforts of
all the workers who collected and organized open language datasets.

Obviously, through training, this model can output character information,
but it does not truly understand the inherent logic and structure of the
language. Therefore, it is clearly inadequate in dealing with any language
task, which is undoubtedly a challenge. Nevertheless, there are infinite
possibilities for future improvement: the model still has a lot of room for
improvement and further research and improvement are needed.
